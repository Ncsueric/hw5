[
  {
    "objectID": "HW5.html",
    "href": "HW5.html",
    "title": "HW5",
    "section": "",
    "text": "Task 1: Conceptual Questions\n\nWhat is the purpose of using cross-validation when fitting a random forest model?\n\nRandom forests have several hyperparameters, such as the number of trees, maximum depth, and number of features to consider at each split. Cross-validation is used to tune these hyperparameters by evaluating the model’s performance for different combinations of hyperparameters and selecting the ones that result in the best performance.\n\nDescribe the bagged tree algorithm.\n\nBootstrap Sampling:\nFrom the original training dataset，generate multiple bootstrap samples. Each bootstrap sample is created by randomly sampling from the original dataset with replacement.\nTrain Individual Trees:\nFor each bootstrap sample, train a decision tree. Since each tree is trained on a different subset of the data, the individual trees will be different from each other.\nAggregate Predictions:\nFor regression problems, the predictions of the individual trees are averaged to produce the final prediction. For classification problems, the predictions of the individual trees are combined using majority voting to produce the final class prediction.\n\nWhat is meant by a general linear model? General Linear Model (GLM) including Continuous response and Allows for both continuous and categorical predictors\nWhen fitting a multiple linear regression model, what does adding an interaction term do? That is, what does it allow the model to do differently as compared to when it is not included in the model?\n\nWhen an interaction term is included, the model accounts for the possibility that the effect of one predictor on the response variable changes depending on the value of another predictor.\n\nWhy do we split our data into a training and test set?\n\nSplitting data into a training and test set is to evaluate the performance of a model and ensure its ability to generalize to new, unseen data.\n\n\nTask 2: Fitting Models\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(caret)\n\nLoading required package: ggplot2\n\n\nLoading required package: lattice\n\nheart &lt;- read.csv(\"heart.csv\")\n\nheart &lt;- heart |&gt;\n  mutate(HeartDiseaseFactor = as.factor(HeartDisease))|&gt;\n  select(-ST_Slope, -HeartDisease)\n\ndummy_vars &lt;- dummyVars(~ Sex + ExerciseAngina + ChestPainType + RestingECG, data = heart)\ndummy_columns &lt;- predict(dummy_vars, newdata = heart)\n\n\nheart &lt;- cbind(heart, dummy_columns)\n\nheart &lt;-heart|&gt;\n  select(-Sex, -ExerciseAngina, -ChestPainType, -RestingECG)\n\nhead(heart)\n\n  Age RestingBP Cholesterol FastingBS MaxHR Oldpeak HeartDiseaseFactor SexF\n1  40       140         289         0   172     0.0                  0    0\n2  49       160         180         0   156     1.0                  1    1\n3  37       130         283         0    98     0.0                  0    0\n4  48       138         214         0   108     1.5                  1    1\n5  54       150         195         0   122     0.0                  0    0\n6  39       120         339         0   170     0.0                  0    0\n  SexM ExerciseAnginaN ExerciseAnginaY ChestPainTypeASY ChestPainTypeATA\n1    1               1               0                0                1\n2    0               1               0                0                0\n3    1               1               0                0                1\n4    0               0               1                1                0\n5    1               1               0                0                0\n6    1               1               0                0                0\n  ChestPainTypeNAP ChestPainTypeTA RestingECGLVH RestingECGNormal RestingECGST\n1                0               0             0                1            0\n2                1               0             0                1            0\n3                0               0             0                0            1\n4                0               0             0                1            0\n5                1               0             0                1            0\n6                1               0             0                1            0\n\n\n\nSplit your data into a training and test set.\n\nset.seed(123)  # for reproducibility\nn &lt;- nrow(heart)\ntrainIndex &lt;- sample(1:n, size = round(0.7 * n), replace = FALSE)\ntrainData &lt;- heart[trainIndex, ]\ntestData &lt;- heart[-trainIndex, ]\n\n\n\nKNN\n\nctrl &lt;- trainControl(method = \"repeatedcv\", number = 10, repeats = 3,\n                     preProcOptions = list(center = TRUE, scale = TRUE))\n\n\nkGrid &lt;- expand.grid(k = 1:40)\n\n\nknn_model &lt;- train(HeartDiseaseFactor ~ ., data = trainData,\n                   method = \"knn\",\n                   trControl = ctrl,\n                   tuneGrid = kGrid)\n\nprint(knn_model$results)\n\n    k  Accuracy     Kappa AccuracySD    KappaSD\n1   1 0.6483494 0.2874253 0.06047908 0.12181870\n2   2 0.6307692 0.2543360 0.04829039 0.09682215\n3   3 0.6696795 0.3295271 0.05350517 0.10698392\n4   4 0.6697276 0.3295687 0.03448267 0.06868285\n5   5 0.6914103 0.3735685 0.04973332 0.09866560\n6   6 0.6764263 0.3455731 0.05048902 0.09819402\n7   7 0.6924119 0.3730608 0.04995035 0.10078567\n8   8 0.6929487 0.3751039 0.04573997 0.09097494\n9   9 0.7017628 0.3927450 0.04830464 0.09745418\n10 10 0.6980849 0.3855527 0.04849292 0.09710689\n11 11 0.7058574 0.4010082 0.05285109 0.10722222\n12 12 0.6975881 0.3851955 0.05309701 0.10800405\n13 13 0.6965144 0.3833778 0.05419769 0.10790303\n14 14 0.6960256 0.3822241 0.05497325 0.11025561\n15 15 0.7006811 0.3928768 0.05410115 0.10800745\n16 16 0.6991506 0.3883482 0.05794690 0.11879957\n17 17 0.7006891 0.3922428 0.05647766 0.11454437\n18 18 0.6996314 0.3902454 0.05995806 0.12193029\n19 19 0.6954407 0.3815883 0.06169819 0.12503075\n20 20 0.6944551 0.3801324 0.05781961 0.11676660\n21 21 0.7027724 0.3970422 0.05220952 0.10674895\n22 22 0.7032933 0.3982380 0.05600348 0.11362025\n23 23 0.7069151 0.4053980 0.05373149 0.11004998\n24 24 0.7022356 0.3965282 0.05421851 0.10970947\n25 25 0.6965144 0.3853692 0.05908003 0.11882225\n26 26 0.7012420 0.3945395 0.05191345 0.10500079\n27 27 0.7064183 0.4055024 0.05408995 0.10814825\n28 28 0.7079808 0.4087572 0.05129461 0.10245151\n29 29 0.7090224 0.4109000 0.05457827 0.10963193\n30 30 0.7090304 0.4106684 0.05084080 0.10209788\n31 31 0.7079487 0.4086206 0.05777617 0.11600024\n32 32 0.7105529 0.4137275 0.05027214 0.10123163\n33 33 0.7100240 0.4131448 0.05414603 0.10873417\n34 34 0.7121154 0.4171987 0.05024915 0.10042816\n35 35 0.7116026 0.4166037 0.04799354 0.09555325\n36 36 0.7116266 0.4166435 0.04804788 0.09605737\n37 37 0.7131651 0.4193122 0.05244750 0.10433788\n38 38 0.7105529 0.4137328 0.04977755 0.09850739\n39 39 0.7105769 0.4144639 0.05209770 0.10279384\n40 40 0.7105609 0.4138116 0.05252439 0.10443220\n\nprint(knn_model$bestTune)\n\n    k\n37 37\n\npredictions &lt;- predict(knn_model, newdata = testData)\nconfusionMatrix(predictions, testData$HeartDiseaseFactor)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0  76  37\n         1  44 118\n                                          \n               Accuracy : 0.7055          \n                 95% CI : (0.6477, 0.7587)\n    No Information Rate : 0.5636          \n    P-Value [Acc &gt; NIR] : 9.203e-07       \n                                          \n                  Kappa : 0.3972          \n                                          \n Mcnemar's Test P-Value : 0.505           \n                                          \n            Sensitivity : 0.6333          \n            Specificity : 0.7613          \n         Pos Pred Value : 0.6726          \n         Neg Pred Value : 0.7284          \n             Prevalence : 0.4364          \n         Detection Rate : 0.2764          \n   Detection Prevalence : 0.4109          \n      Balanced Accuracy : 0.6973          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\n###　Logistic Regression\n\nmodel &lt;- train(HeartDiseaseFactor ~ ., data = heart,\n                 method = \"glm\", family = \"binomial\")\nsummary(model)\n\n\nCall:\nNULL\n\nCoefficients: (4 not defined because of singularities)\n                  Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)       1.444521   1.266906   1.140 0.254205    \nAge               0.014741   0.011831   1.246 0.212775    \nRestingBP         0.002637   0.005366   0.491 0.623101    \nCholesterol      -0.003377   0.001011  -3.340 0.000837 ***\nFastingBS         1.165555   0.247504   4.709 2.49e-06 ***\nMaxHR            -0.014290   0.004395  -3.251 0.001148 ** \nOldpeak           0.631153   0.106410   5.931 3.00e-09 ***\nSexF             -1.155396   0.246196  -4.693 2.69e-06 ***\nSexM                    NA         NA      NA       NA    \nExerciseAnginaN  -1.298818   0.220665  -5.886 3.96e-09 ***\nExerciseAnginaY         NA         NA      NA       NA    \nChestPainTypeASY  1.190169   0.390069   3.051 0.002279 ** \nChestPainTypeATA -0.783320   0.442309  -1.771 0.076564 .  \nChestPainTypeNAP -0.299323   0.402385  -0.744 0.456954    \nChestPainTypeTA         NA         NA      NA       NA    \nRestingECGLVH     0.383020   0.320736   1.194 0.232404    \nRestingECGNormal  0.196094   0.264754   0.741 0.458897    \nRestingECGST            NA         NA      NA       NA    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1262.14  on 917  degrees of freedom\nResidual deviance:  710.54  on 904  degrees of freedom\nAIC: 738.54\n\nNumber of Fisher Scoring iterations: 5\n\nctrl &lt;- trainControl(method = \"repeatedcv\", number = 10)\n\n\nmodel1 &lt;- train(HeartDiseaseFactor ~ ., data = trainData,\n                 method = \"glm\", family = \"binomial\",\n                 trControl = ctrl)\n  \nprint(model1)\n\nGeneralized Linear Model \n\n643 samples\n 17 predictor\n  2 classes: '0', '1' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold, repeated 1 times) \nSummary of sample sizes: 578, 579, 579, 578, 579, 578, ... \nResampling results:\n\n  Accuracy   Kappa    \n  0.8132452  0.6219691\n\nmodel2 &lt;- train(HeartDiseaseFactor ~Cholesterol +FastingBS +Oldpeak+SexF+ExerciseAnginaN, data = trainData,\n                 method = \"glm\", family = \"binomial\",\n                 trControl = ctrl)\n  \nprint(model2)\n\nGeneralized Linear Model \n\n643 samples\n  5 predictor\n  2 classes: '0', '1' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold, repeated 1 times) \nSummary of sample sizes: 579, 578, 579, 579, 579, 579, ... \nResampling results:\n\n  Accuracy   Kappa    \n  0.7776202  0.5525251\n\nmodel3 &lt;- train(HeartDiseaseFactor ~Cholesterol +FastingBS +Oldpeak+SexF+ExerciseAnginaN+MaxHR+ChestPainTypeASY, data = trainData,\n                 method = \"glm\", family = \"binomial\",\n                 trControl = ctrl)\n  \nprint(model3)\n\nGeneralized Linear Model \n\n643 samples\n  7 predictor\n  2 classes: '0', '1' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold, repeated 1 times) \nSummary of sample sizes: 579, 578, 579, 579, 578, 579, ... \nResampling results:\n\n  Accuracy   Kappa   \n  0.8165144  0.628701\n\npredictions &lt;- predict(model3, newdata = testData)\nconfusionMatrix(predictions, testData$HeartDiseaseFactor)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0  95  27\n         1  25 128\n                                          \n               Accuracy : 0.8109          \n                 95% CI : (0.7595, 0.8554)\n    No Information Rate : 0.5636          \n    P-Value [Acc &gt; NIR] : &lt;2e-16          \n                                          \n                  Kappa : 0.6163          \n                                          \n Mcnemar's Test P-Value : 0.8897          \n                                          \n            Sensitivity : 0.7917          \n            Specificity : 0.8258          \n         Pos Pred Value : 0.7787          \n         Neg Pred Value : 0.8366          \n             Prevalence : 0.4364          \n         Detection Rate : 0.3455          \n   Detection Prevalence : 0.4436          \n      Balanced Accuracy : 0.8087          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\n###　Tree Models #### classification tree model\n\ntrain_control &lt;- trainControl(method = \"cv\", number = 10)\n\n\ncp_grid &lt;- expand.grid(cp = seq(0, 0.1, by = 0.001))\n\nmodel &lt;- train(HeartDiseaseFactor ~Cholesterol +FastingBS +Oldpeak+SexF+ExerciseAnginaN+MaxHR+ChestPainTypeASY, data = trainData, method = \"rpart\", \n               trControl = train_control, tuneGrid = cp_grid)\n\npredictions &lt;- predict(model, newdata = testData)\nconfusionMatrix(predictions, testData$HeartDiseaseFactor)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0  74  18\n         1  46 137\n                                          \n               Accuracy : 0.7673          \n                 95% CI : (0.7128, 0.8159)\n    No Information Rate : 0.5636          \n    P-Value [Acc &gt; NIR] : 1.494e-12       \n                                          \n                  Kappa : 0.5141          \n                                          \n Mcnemar's Test P-Value : 0.0007382       \n                                          \n            Sensitivity : 0.6167          \n            Specificity : 0.8839          \n         Pos Pred Value : 0.8043          \n         Neg Pred Value : 0.7486          \n             Prevalence : 0.4364          \n         Detection Rate : 0.2691          \n   Detection Prevalence : 0.3345          \n      Balanced Accuracy : 0.7503          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\n\n\nrandom forest\n\ntrain_control &lt;- trainControl(method = \"cv\", number = 10)\n\n\nmtry_grid &lt;- expand.grid(mtry = 1:7)\n\n\nmodel &lt;- train(HeartDiseaseFactor ~Cholesterol +FastingBS +Oldpeak+SexF+ExerciseAnginaN+MaxHR+ChestPainTypeASY, data = trainData, method = \"rf\", \n               trControl = train_control, tuneGrid = mtry_grid )\n\npredictions &lt;- predict(model, newdata = testData)\nconfusionMatrix(predictions, testData$HeartDiseaseFactor)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0  90  18\n         1  30 137\n                                          \n               Accuracy : 0.8255          \n                 95% CI : (0.7753, 0.8684)\n    No Information Rate : 0.5636          \n    P-Value [Acc &gt; NIR] : &lt;2e-16          \n                                          \n                  Kappa : 0.6411          \n                                          \n Mcnemar's Test P-Value : 0.1124          \n                                          \n            Sensitivity : 0.7500          \n            Specificity : 0.8839          \n         Pos Pred Value : 0.8333          \n         Neg Pred Value : 0.8204          \n             Prevalence : 0.4364          \n         Detection Rate : 0.3273          \n   Detection Prevalence : 0.3927          \n      Balanced Accuracy : 0.8169          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\n####　boosted tree\n\ntrain_control &lt;- trainControl(method = \"cv\", number = 10)\n\ntuning_grid &lt;- expand.grid(\n  n.trees = c(25, 50, 100, 200),\n  interaction.depth = c(1, 2, 3),\n  shrinkage = 0.1,\n  n.minobsinnode = 10\n)\n\nmodel &lt;- train(HeartDiseaseFactor ~Cholesterol +FastingBS +Oldpeak+SexF+ExerciseAnginaN+MaxHR+ChestPainTypeASY, data = trainData, method = \"gbm\",\n               trControl = train_control, tuneGrid = tuning_grid,\n               verbose = FALSE)\n\npredictions &lt;- predict(model, newdata = testData)\nconfusionMatrix(predictions, testData$HeartDiseaseFactor)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0  93  25\n         1  27 130\n                                          \n               Accuracy : 0.8109          \n                 95% CI : (0.7595, 0.8554)\n    No Information Rate : 0.5636          \n    P-Value [Acc &gt; NIR] : &lt;2e-16          \n                                          \n                  Kappa : 0.6149          \n                                          \n Mcnemar's Test P-Value : 0.8897          \n                                          \n            Sensitivity : 0.7750          \n            Specificity : 0.8387          \n         Pos Pred Value : 0.7881          \n         Neg Pred Value : 0.8280          \n             Prevalence : 0.4364          \n         Detection Rate : 0.3382          \n   Detection Prevalence : 0.4291          \n      Balanced Accuracy : 0.8069          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\n\n\n\nWrap up\nrandom forest is the best model with Accuracy rate: 0.8255"
  }
]